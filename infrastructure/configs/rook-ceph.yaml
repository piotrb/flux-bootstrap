## BLOCK
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph # namespace:cluster
spec:
  failureDomain: osd
  replicated:
    size: 1
    # Disallow setting pool with replica 1, this could lead to data loss without recovery.
    # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
    requireSafeReplicaSize: false
    # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
    # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
    #targetSizeRatio: .5
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com # csi-provisioner-name
parameters:
  # clusterID is the namespace where the rook cluster is running
  # If you change this namespace, also change the namespace below where the secret namespaces are defined
  clusterID: rook-ceph # namespace:cluster

  # If you want to use erasure coded pool with RBD, you need to create
  # two pools. one erasure coded and one replicated.
  # You need to specify the replicated pool here in the `pool` parameter, it is
  # used for the metadata of the images.
  # The erasure coded pool must be set as the `dataPool` parameter below.
  #dataPool: ec-data-pool
  pool: replicapool

  # RBD image format. Defaults to "2".
  imageFormat: "2"

  # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
  imageFeatures: layering

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster
  # Specify the filesystem type of the volume. If not specified, csi-provisioner
  # will set default as `ext4`.
  csi.storage.k8s.io/fstype: ext4
# uncomment the following to use rbd-nbd as mounter on supported nodes
#mounter: rbd-nbd
allowVolumeExpansion: true
reclaimPolicy: Delete

# ## FILESYSTEM
# ---
# apiVersion: ceph.rook.io/v1
# kind: CephFilesystem
# metadata:
#   name: myfs
#   namespace: rook-ceph # namespace:cluster
# spec:
#   metadataPool:
#     replicated:
#       size: 1
#       requireSafeReplicaSize: false
#   dataPools:
#     - name: replicated
#       failureDomain: osd
#       replicated:
#         size: 1
#         requireSafeReplicaSize: false
#   preserveFilesystemOnDelete: false
#   metadataServer:
#     activeCount: 1
#     activeStandby: true
# ---
# # create default csi subvolume group
# apiVersion: ceph.rook.io/v1
# kind: CephFilesystemSubVolumeGroup
# metadata:
#   name: myfs-csi # lets keep the svg crd name same as `filesystem name + csi` for the default csi svg
#   namespace: rook-ceph # namespace:cluster
# spec:
#   # The name of the subvolume group. If not set, the default is the name of the subvolumeGroup CR.
#   name: csi
#   # filesystemName is the metadata name of the CephFilesystem CR where the subvolume group will be created
#   filesystemName: myfs
#   # reference https://docs.ceph.com/en/latest/cephfs/fs-volumes/#pinning-subvolumes-and-subvolume-groups
#   # only one out of (export, distributed, random) can be set at a time
#   # by default pinning is set with value: distributed=1
#   # for disabling default values set (distributed=0)
#   pinning:
#     distributed: 1            # distributed=<0, 1> (disabled=0)
#     # export:                 # export=<0-256> (disabled=-1)
#     # random:                 # random=[0.0, 1.0](disabled=0.0)

# ## OBJECT
# ---
# apiVersion: ceph.rook.io/v1
# kind: CephObjectStore
# metadata:
#   name: my-store
#   namespace: rook-ceph # namespace:cluster
# spec:
#   metadataPool:
#     replicated:
#       size: 1
#   dataPool:
#     replicated:
#       size: 1
#   preservePoolsOnDelete: false
#   gateway:
#     port: 80
#     # securePort: 443
#     instances: 1